---
title: Moissonnage du Web avec R
slug: webscrapingfr
execute:
  cache: false
  error: true
format: hugo-md
---

**16 février 2023, 13h30 à 14h50 HNE**

**Présenté par**: Pier-Luc St-Onge

**Durée**: 80 minutes

**Description**: Internet est non seulement un trésor riche en informations, mais une grande portion y est
accessible publiquement, ce qui est propice à une utilisation pour la recherche. Cependant, extraire
l’information de pages Web et la formater pour analyse peut rapidement devenir une tâche fastidieuse. Les
outils de moissonnage du Web permettent d’automatiser en partie ce processus et le langage R est populaire
pour réaliser cette tâche. Dans cet atelier, nous allons donc vous guider à travers un exemple simple
utilisant le module rvest.

Inscrivez-vous [ici](https://www.eventbrite.ca/e/512943125697){target="_blank"}

The same workshop [in English](/webscraping).

#### Biographie

Diplômé au baccalauréat et à la maîtrise en génie logiciel et génie informatique, **Pier-Luc St-Onge** a
travaillé pendant cinq ans pour différents laboratoires de recherche avant de rejoindre Calcul Québec en
mai 2013. Dans son projet de recherche, il s’était spécialisé en vision par ordinateur avec OpenCV. À Calcul
Québec, il fait partie de l’équipe d’analystes offrant du soutien aux utilisateurs des ressources de calcul.

<!-- {{< vimeo 690948795 >}} -->
<!-- <br> -->

<!-- - [Watch this session on Vimeo](https://vimeo.com/690948795) -->

{{<br>}}

-----

## À propos

Le matériel ci-dessous est une traduction et adaptation du matériel original
[*Web scraping with R*](https://mint.westdri.ca/r/webscraping.html){target="_blank"}
de Marie-Hélène Burle sur le
[site d'ateliers de WestDRI](https://mint.westdri.ca/){target="_blank"}.

Pour cet atelier, nous allons utiliser un serveur RStudio éphémère.
Pour y accéder, il faut se connecter à un portail JupyterHub;
les détails sont donnés en atelier.

Le serveur RStudio est déjà configuré avec les deux modules suivants :

* [rvest](https://cran.r-project.org/web/packages/rvest/index.html){target="_blank"}
* [tibble](https://cran.r-project.org/web/packages/tibble/index.html){target="_blank"}

Si vous préférez exécuter les exemples sur votre propre ordinateur,
vous devez les installer avec `install.packages()`.

## HTML et CSS

Le [*HyperText Markup Language*](https://fr.wikipedia.org/wiki/Hypertext_Markup_Language){target="_blank"}
(ou HTML) est le langage de balisage standard pour représenter des pages Web.
Il permet, entre autres, d'encoder la structure d'une page Web et
le formatage de son contenu.
Pour ce qui est des règles de formatage réutilisées par plusieurs
pages HTML, il est possible de les sauvegarder dans des fichiers
[*Cascading Style Sheets*](https://fr.wikipedia.org/wiki/Feuilles_de_style_en_cascade){target="_blank"}
(ou CSS).

Le HTML utilise des balises de la forme suivante :

```{.html}
<balise>Du contenu</balise>
```

Certaines balises ont aussi des attributs :

```{.html}
<balise nom_attribut="valeur attribut">Du contenu</balise>
```

{{<ex>}}
Exemples réels :
{{</ex>}}

Structure du site :

```{.html}
<h2 id="ancre">Ceci est un titre de niveau 2</h2>
<p>Ceci est un paragraphe. Cliquez ce
  <a href="https://une.adresse/page.html#ancre">lien</a>.
</p>
```

Formatage :

```{.html}
<p class="texte_rouge">
  <strong>Pour du texte en caractères gras</strong>
</p>
```

## Moissonnage du Web

Le moissonnage du Web consiste à utiliser un ensemble d'outils afin
d'extraire automatiquement de l'information directement d'Internet.

Alors que la plupart des données sur Internet sont disponibles
publiquement, il peut être illégal de moissonner certains sites.
En effet, vous devriez toujours regarder la politique d'utilisation d'un
site Web avant de tenter toute extraction d'information par moissonnage.
Notez aussi que certains sites peuvent vous bloquer si vous leur envoyez
un trop grand nombre de requêtes HTTP dans une courte période de temps.
Par conséquent, si vous planifiez effectuer du moissonnage à grande
échelle, vous devriez considérer l'utilisation du module R
[`polite`](https://dmi3kno.github.io/polite/){target="_blank"}.

## Un site Web en exemple

### Notre objectif

Nous allons utiliser
[**ce site Web**](https://trace.tennessee.edu/utk_graddiss/index.html){target="_blank"}
de [l'Université du Tennessee](https://www.utk.edu/){target="_blank"},
car il présente une interface de base de données
dont le code HTML est facile à décortiquer.
**Notre objectif consiste donc à extraire de ce site la date de publication,
le domaine de recherche et le nom du directeur ou de la directrice de
recherche pour chacune des thèses de doctorat de cette université.**
Ces informations doivent ensuite être colligées dans un *data frame*.

> Note : pour cet atelier, nous nous limiterons à la première
  page qui contient les liens des 100 thèses les plus récentes.
  Si vous vouliez extraire les données de toutes les thèses, il vous
  faudrait envoyer une requête HTTP pour chaque page de la base de données.

### Méthodologie

Avant de se lancer dans la programmation, il est recommandé
de réfléchir aux grandes étapes de notre moissonnage.
Étant donné la structure du site Web, la création du *data frame* avec
les données des thèses de la première page se fera en deux étapes :

1. À partir de
  [la première page](https://trace.tennessee.edu/utk_graddiss/index.html){target="_blank"}
  de la base de données, nous voulons colliger une liste
  d'hyperliens menant vers les pages de chaque thèse.
2. Avec ces hyperliens, nous voulons moissonner les pages
  correspondantes afin d'y extraire la valeur des champs
  *Date of Award*, *Major* et *Major Professor* de chaque thèse.

### Module R

Pour les deux grandes étapes ci-dessus, nous allons utiliser le module R
[`rvest`](https://cran.r-project.org/web/packages/rvest/index.html){target="_blank"}
qui fait partie du [tidyverse](https://www.tidyverse.org/){target="_blank"},
un ensemble moderne de modules R.
Il s'agit d'un module influencé par le populaire module Python
[Beautiful Soup](https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)){target="_blank"}
et qui facilite le moissonnage du Web via le langage R.

Chargeons-le dans notre session R :

```{r}
library(rvest)
```

### Obtenir les données HTML

Tel que mentionné ci-dessus, notre site en exemple est la
[base de données de thèses de doctorat](https://trace.tennessee.edu/utk_graddiss/index.html){target="_blank"}
de l'Université du Tennessee.

Assignons l'adresse du site (une chaîne de caractères) à une variable :

```{r}
adresse <- "https://trace.tennessee.edu/utk_graddiss/index.html"
```

L'étape suivante consiste à obtenir les données HTML de cette page :

```{r}
html <- read_html(adresse)
```

Regardons un aperçu de ces données HTML encore à l'état brut :

```{r}
html
```

### Extraction des données pertinentes

#### Inspection du code HTML

Le code HTML contient bel et bien les données qui nous intéressent,
mais elles sont mélangées avec plusieurs balises HTML, des instructions
de formatage et d'autres données inutiles pour notre objectif.
Nous devons donc extraire ces données et les regrouper dans un format pratique.

La première étape consiste à trouver les branches de balises,
souvent enrichies d'un identifiant ou d'éléments de CSS,
qui contiennent les données que nous voulons.
Pour ce faire, vous pouvez utiliser l'inspecteur intégré
de votre navigateur Web sur la page de thèses.
Par exemple, en pointant un hyperlien avec la souris :

* **Firefox** : bouton droit de la souris et *Inspecter*
  * L'arborescence se trouve au bas de la fenêtre
* **Google Chrome** : bouton droit de la souris et *Inspecter*
  * L'arborescence se trouve à la droite de la fenêtre
* **Microsoft Edge** : bouton droit de la souris et *Inspecter*
  * L'arborescence se trouve à la droite de la fenêtre
* **Safari** : bouton droit de la souris et *Inspecter l'élément*
  * Note: il faut au préalable activer le menu *Développement* dans les
    paramètres avancés.

> Voir aussi le [SelectorGadget](https://selectorgadget.com/){target="_blank"}.

On remarque donc que toutes les adresses qui nous intéressent se trouvent
dans l'attribut `href` des balises `<a>` qui sont elles-même dans des
paragraphes `<p>` dont la classe CSS porte le nom `article-listing`.

#### Extracting a single link

It is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let's test our method on the first dissertation.

First, we need to extract the first URL. The function `html_element()` from the package `rvest` does exactly this. Let's assign the result to an object that we will call `test`:

```{r}
test <- html %>% html_element(".article-listing")
```

:::{.note}

`%>%` is a pipe from the [magrittr](https://magrittr.tidyverse.org/) tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:

```{.r}
test <- html_element(html, ".article-listing")
```

:::

Our new object is a list:

```{r}
typeof(test)
```

Let's print it:

```{r}
test
```

The link is in there, so we successfully extracted the correct element, but we need to do more cleaning.

As you can see from printing `test`, the link is in an `a` anchor element. Let's extract it by running the function `html_element()` again:

```{r}
a_test <- test %>% html_element("a")
a_test
```

:::{.note}

An alternative method is to use the `html_children()` function which gets the children element (the element inside an element):

```{r}
test %>% html_children()
```

:::

This is much better, but we still need to extract the `href` attribute:

```{r}
link_test <- a_test %>% html_attr("href")
link_test
```

This is our link.

```{r}
str(link_test)
```

It is saved in a character vector, which is perfect.

#### Getting data from our test link

Now that we have the URL for the first dissertation information page, we want to extract the date, major, and principal investigator (PI) for that dissertation.

We just saw that `link_test` is a character vector representing a URL. We know how to deal with this.

The first thing to do—as we did earlier with the database site—is to read in the html data. Let's assign it to a new object that we will call `html_test`:

```{r}
html_test <- read_html(link_test)
html_test
```

Now, we want to extract the publication date. Thanks to the [SelectorGadget](https://selectorgadget.com/), we can see that it is in the element `#publication_date p`. While earlier we wanted a link (i.e. part of the CSS formatting data), here we just want the text. In this case, we extract the element with `html_element()` (as before) and pass the result to `html_text2()`:

```{r}
date_test <- html_test %>%
  html_element("#publication_date p") %>%
  html_text2()
```

:::{.note}

`html_text2()` extracts the text part of an element and formats it nicely.

Note the difference with what we did earlier to extract the link: if we had used `html_text2()` then we would have gotten the text part of the link—that is "The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods"—rather than the link itself.

:::

Let's verify that our `date` object indeed contains the date:

```{r}
date_test
```

We also want the major for this thesis. The [SelectorGadget](https://selectorgadget.com/) allows us to find that this time, it is the `#department p` element that we need. Let's extract it in the same fashion:

```{r}
major_test <- html_test %>%
  html_element("#department p") %>%
  html_text2()
major_test
```

And for the PI, we need the `#advisor1 p` element:

```{r}
pi_test <- html_test %>%
  html_element("#advisor1 p") %>%
  html_text2()
pi_test
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

Try using the [SelectorGadget](https://selectorgadget.com/) to identify the element necessary to extract the abstract of this dissertation.

Now, write the code to extract it.

:::

We now have the date, major, and PI for the first dissertation. We could create a matrix with them by passing them as arguments to `cbind()`:

```{r}
result_test <- cbind(date_test, major_test, pi_test)
result_test
```

#### Extracting all links

Now that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.

Instead of using `html_element()`, this time we will use `html_elements()` which extracts *all* matching elements (instead of just the first one):

```{r}
dat <- html %>% html_elements(".article-listing")
dat
```

```{r}
typeof(dat)
length(dat)
typeof(dat[[1]])
```

We now have a list of lists.

As we did for a single link, we want to extract all the links to have a list of links. We will do this using a loop.

Before running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.

So let's initialize an empty list:

```{r}
list_links <- vector("list", length(dat))
```

Let's have a look at one element of our list (the second one for instance):

```{r}
list_links[[2]]
```

We now have an empty list of the appropriate size. We can run our loop:

```{r}
for (i in seq_along(dat)) {
  list_links[[i]] <- dat[[i]] %>%
    html_element("a") %>%
    html_attr("href")
}
```

Let's print again the second element of our list to make sure all looks good:

```{r}
list_links[[2]]
```

We have a character vector with one link. That's great! `list_links` is a list of links (in the form of character vectors) as we wanted.

#### Getting the data from the list of links

We will now extract the data (date, major, and PI) for all links in our list.

Again, before running a for loop, we need to allocate memory first by creating an empty container:

```{r}
list_data <- vector("list", length(list_links))
```

We move the code we tested for a single dissertation inside a loop and we add one result to the `list_data` list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don't want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration:

```{.r}
for (i in seq_along(list_links)) {
  html <- read_html(list_links[[i]])
  date <- html %>%
    html_element("#publication_date p") %>%
    html_text2()
  major <- html %>%
    html_element("#department p") %>%
    html_text2()
  pi <- html %>%
    html_element("#advisor1 p") %>%
    html_text2()
  Sys.sleep(0.1)  # Add a little delay
  list_data[[i]] <- cbind(date, major, pi)
}
```

Let's make sure all looks good by printing the second element of `list_data`:

```{r}
list_data[[2]]
```

All looking good, so let's turn this big list into a dataframe:

```{r}
result <- do.call(rbind.data.frame, list_data)
```

`result` is a long dataframe, so we will only print the first few elements:

```{r}
head(result)
```

If you like the tidyverse, you can turn it into a tibble:

```{r}
result <- result %>% tibble::as_tibble()
```

:::{.note}

The notation `tibble::as_tibble()` means that we are using the function `as_tibble()` from the package [tibble](https://tibble.tidyverse.org/). A tibble is the [tidyverse](https://www.tidyverse.org/) version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don't have to use `head()` when printing long dataframes:

```{r}
result
```

:::

We can rename the headers:

```{r}
names(result) <- c("Date", "Major", "PI")
```

This is what our final result looks like:

```{r}
result
```

## Functions recap

Below is a recapitulation of the `rvest` functions we have used today:

| Functions | Usage |
|-----------|-------|
| `read_html()` | Read in HTML data |
| `html_element()` | Extract first matching element |
| `html_elements()` | Extract all matching elements |
| `html_children()` | Get children element |
| `html_attr()` | Get an attribute |
| `html_text2()` | Retrieve text |
